

<!doctype html>
<html lang="en" class="no-js">
  <head>
    
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width,initial-scale=1">
      
      
      
      <link rel="icon" href="">
    
    
      
        <title>Model quantization and compilation using TFLite and TVM - Kenning</title>
      
    
    
      
        
        
      
      

    
    
    
      
        
        
        <style>:root{--md-text-font:"Roboto";--md-code-font:"Roboto Mono"}</style>
      
    
        <link rel="stylesheet" type="text/css" href="../_static/sphinx_immaterial_theme.c5589c0bd87d933a4.min.css?v=eeeb54bc" />
        <link rel="stylesheet" type="text/css" href="../_static/css/bokeh.css?v=8bee089c" />
        <link rel="stylesheet" type="text/css" href="../_static/css/compatibility.css?v=5eb45fba" />
    <script>__md_scope=new URL("..",location),__md_hash=e=>[...e].reduce((e,_)=>(e<<5)-e+_.charCodeAt(0),0),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script>
    
      

    
    
    
  </head>
  
  
    
    
      
    
    
    
    
    <body dir="ltr" data-md-color-scheme="slate" data-md-color-primary="teal" data-md-color-accent="deep-orange">
  
    
    
      <script>var palette=__md_get("__palette");if(palette&&"object"==typeof palette.color)for(var key of Object.keys(palette.color))document.body.setAttribute("data-md-color-"+key,palette.color[key])</script>
    
    <input class="md-toggle" data-md-toggle="drawer" type="checkbox" id="__drawer" autocomplete="off">
    <input class="md-toggle" data-md-toggle="search" type="checkbox" id="__search" autocomplete="off">
    <label class="md-overlay" for="__drawer"></label>
    <div data-md-component="skip">
      
    </div>
    <div data-md-component="announce">
      
    </div>
    
    
      

<header class="md-header" data-md-component="header">
  <nav class="md-header__inner md-grid" aria-label="Header">
    <a href="../index.html" title="Kenning" class="md-header__button md-logo" aria-label="Kenning" data-md-component="logo">
      <img src="../_static/white.svg" alt="logo">
    </a>
    <label class="md-header__button md-icon" for="__drawer">
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M3 6h18v2H3V6m0 5h18v2H3v-2m0 5h18v2H3v-2Z"/></svg>
    </label>
    <div class="md-header__title" data-md-component="header-title">
      <div class="md-header__ellipsis">
        <div class="md-header__topic">
          <span class="md-ellipsis">
            Kenning
          </span>
        </div>
        <div class="md-header__topic" data-md-component="header-topic">
          <span class="md-ellipsis">
            
              Model quantization and compilation using TFLite and TVM
            
          </span>
        </div>
      </div>
    </div>
    
      <form class="md-header__option" data-md-component="palette">
        
          
          
          <input class="md-option" data-md-color-media="" data-md-color-scheme="slate" data-md-color-primary="teal" data-md-color-accent="deep-orange"  aria-label="Switch to light mode"  type="radio" name="__palette" id="__palette_1">
          
            <label class="md-header__button md-icon" title="Switch to light mode" for="__palette_2" hidden>
              <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 7a5 5 0 0 1 5 5 5 5 0 0 1-5 5 5 5 0 0 1-5-5 5 5 0 0 1 5-5m0 2a3 3 0 0 0-3 3 3 3 0 0 0 3 3 3 3 0 0 0 3-3 3 3 0 0 0-3-3m0-7 2.39 3.42C13.65 5.15 12.84 5 12 5c-.84 0-1.65.15-2.39.42L12 2M3.34 7l4.16-.35A7.2 7.2 0 0 0 5.94 8.5c-.44.74-.69 1.5-.83 2.29L3.34 7m.02 10 1.76-3.77a7.131 7.131 0 0 0 2.38 4.14L3.36 17M20.65 7l-1.77 3.79a7.023 7.023 0 0 0-2.38-4.15l4.15.36m-.01 10-4.14.36c.59-.51 1.12-1.14 1.54-1.86.42-.73.69-1.5.83-2.29L20.64 17M12 22l-2.41-3.44c.74.27 1.55.44 2.41.44.82 0 1.63-.17 2.37-.44L12 22Z"/></svg>
            </label>
          
        
          
          
          <input class="md-option" data-md-color-media="" data-md-color-scheme="default" data-md-color-primary="teal" data-md-color-accent="deep-orange"  aria-label="Switch to dark mode"  type="radio" name="__palette" id="__palette_2">
          
            <label class="md-header__button md-icon" title="Switch to dark mode" for="__palette_1" hidden>
              <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="m17.75 4.09-2.53 1.94.91 3.06-2.63-1.81-2.63 1.81.91-3.06-2.53-1.94L12.44 4l1.06-3 1.06 3 3.19.09m3.5 6.91-1.64 1.25.59 1.98-1.7-1.17-1.7 1.17.59-1.98L15.75 11l2.06-.05L18.5 9l.69 1.95 2.06.05m-2.28 4.95c.83-.08 1.72 1.1 1.19 1.85-.32.45-.66.87-1.08 1.27C15.17 23 8.84 23 4.94 19.07c-3.91-3.9-3.91-10.24 0-14.14.4-.4.82-.76 1.27-1.08.75-.53 1.93.36 1.85 1.19-.27 2.86.69 5.83 2.89 8.02a9.96 9.96 0 0 0 8.02 2.89m-1.64 2.02a12.08 12.08 0 0 1-7.8-3.47c-2.17-2.19-3.33-5-3.49-7.82-2.81 3.14-2.7 7.96.31 10.98 3.02 3.01 7.84 3.12 10.98.31Z"/></svg>
            </label>
          
        
      </form>
    
    
    
      <div class="md-header__button">
        <a href="../kenning.pdf" title="PDF - kenning.pdf">
          <div class="md-icon">
            
            <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 384 512"><!--! Font Awesome Free 6.2.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2022 Fonticons, Inc.--><path d="M64 0C28.7 0 0 28.7 0 64v384c0 35.3 28.7 64 64 64h256c35.3 0 64-28.7 64-64V160H256c-17.7 0-32-14.3-32-32V0H64zm192 0v128h128L256 0zM64 224h24c30.9 0 56 25.1 56 56s-25.1 56-56 56h-8v32c0 8.8-7.2 16-16 16s-16-7.2-16-16V240c0-8.8 7.2-16 16-16zm24 80c13.3 0 24-10.7 24-24s-10.7-24-24-24h-8v48h8zm72-64c0-8.8 7.2-16 16-16h24c26.5 0 48 21.5 48 48v64c0 26.5-21.5 48-48 48h-24c-8.8 0-16-7.2-16-16V240zm32 112h8c8.8 0 16-7.2 16-16v-64c0-8.8-7.2-16-16-16h-8v96zm96-128h48c8.8 0 16 7.2 16 16s-7.2 16-16 16h-32v32h32c8.8 0 16 7.2 16 16s-7.2 16-16 16h-32v48c0 8.8-7.2 16-16 16s-16-7.2-16-16V240c0-8.8 7.2-16 16-16z"/></svg>
          </div>
        </a>
      </div>
    
    
      <label class="md-header__button md-icon" for="__search">
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.516 6.516 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5Z"/></svg>
      </label>
      <div class="md-search" data-md-component="search" role="dialog">
  <label class="md-search__overlay" for="__search"></label>
  <div class="md-search__inner" role="search">
    <form class="md-search__form" name="search">
      <input type="text" class="md-search__input" name="query" aria-label="Search" placeholder="Search" autocapitalize="off" autocorrect="off" autocomplete="off" spellcheck="false" data-md-component="search-query" required>
      <label class="md-search__icon md-icon" for="__search">
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.516 6.516 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5Z"/></svg>
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11h12Z"/></svg>
      </label>
      <nav class="md-search__options" aria-label="Search">
        
        <button type="reset" class="md-search__icon md-icon" title="Clear" aria-label="Clear" tabindex="-1">
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12 19 6.41Z"/></svg>
        </button>
      </nav>
      
    </form>
    <div class="md-search__output">
      <div class="md-search__scrollwrap" data-md-scrollfix>
        <div class="md-search-result" data-md-component="search-result">
          <div class="md-search-result__meta">
            Initializing search
          </div>
          <ol class="md-search-result__list"></ol>
        </div>
      </div>
    </div>
  </div>
</div>
    
    
      <div class="md-header__source">
        <a href="https://github.com/antmicro/kenning" title="Go to repository" class="md-source" data-md-component="source">
  <div class="md-source__icon md-icon">
    
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 496 512"><!--! Font Awesome Free 6.2.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2022 Fonticons, Inc.--><path d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6zm-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3zm44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9zM244.8 8C106.1 8 0 113.3 0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8zM97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1zm-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7zm32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1zm-11.4-14.7c-1.6 1-1.6 3.6 0 5.9 1.6 2.3 4.3 3.3 5.6 2.3 1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2z"/></svg>
  </div>
  <div class="md-source__repository">
    antmicro/kenning
  </div>
</a>
      </div>
    
  </nav>
  
</header>
    
    <div class="md-container" data-md-component="container">
      
      
      
      
        
          
        
      
      <main class="md-main" data-md-component="main">
        <div class="md-main__inner md-grid">
          
            
              
              <div class="md-sidebar md-sidebar--primary" data-md-component="sidebar" data-md-type="navigation" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    


  

<nav class="md-nav md-nav--primary md-nav--integrated" aria-label="Navigation" data-md-level="0">
  <label class="md-nav__title" for="__drawer">
    <a href="../index.html" title="Kenning" class="md-nav__button md-logo" aria-label="Kenning" data-md-component="logo">
      <img src="../_static/white.svg" alt="logo">
    </a>
    Kenning
  </label>
  
    <div class="md-nav__source">
      <a href="https://github.com/antmicro/kenning" title="Go to repository" class="md-source" data-md-component="source">
  <div class="md-source__icon md-icon">
    
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 496 512"><!--! Font Awesome Free 6.2.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2022 Fonticons, Inc.--><path d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6zm-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3zm44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9zM244.8 8C106.1 8 0 113.3 0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8zM97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1zm-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7zm32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1zm-11.4-14.7c-1.6 1-1.6 3.6 0 5.9 1.6 2.3 4.3 3.3 5.6 2.3 1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2z"/></svg>
  </div>
  <div class="md-source__repository">
    antmicro/kenning
  </div>
</a>
    </div>
  
  <ul class="md-nav__list" data-md-scrollfix>
    
      
      
      

  
  
  
  
    <li class="md-nav__item">
      <a href="../introduction.html" class="md-nav__link">
        <span class="md-ellipsis">Introduction</span>
      </a>
    </li>
  

    
      
      
      

  
  
  
  
    <li class="md-nav__item">
      <a href="../project-readme.html" class="md-nav__link">
        <span class="md-ellipsis">Kenning</span>
      </a>
    </li>
  

    
      
      
      

  
  
  
  
    <li class="md-nav__item">
      <a href="../dl-deployment-stack.html" class="md-nav__link">
        <span class="md-ellipsis">Deep Learning deployment stack</span>
      </a>
    </li>
  

    
      
      
      

  
  
  
  
    <li class="md-nav__item">
      <a href="../json-scenarios.html" class="md-nav__link">
        <span class="md-ellipsis">Defining optimization pipelines in Kenning</span>
      </a>
    </li>
  

    
      
      
      

  
  
  
  
    <li class="md-nav__item">
      <a href="../cmd-usage.html" class="md-nav__link">
        <span class="md-ellipsis">Using Kenning via command line arguments</span>
      </a>
    </li>
  

    
      
      
      

  
  
    
  
  
    
    <li class="md-nav__item md-nav__item--active md-nav__item--nested">
      
      
        <input class="md-nav__toggle md-toggle" data-md-toggle="__nav_6" type="checkbox" id="__nav_6" checked>
      
      
      
        
        <div class="md-nav__link md-nav__link--index ">
          <a href="../kenning-gallery.html"><span class="md-ellipsis">Kenning gallery of use cases</span></a>
          
            <label for="__nav_6">
              <span class="md-nav__icon md-icon"></span>
            </label>
          
        </div>
      
      <nav class="md-nav" aria-label="Kenning gallery of use cases" data-md-level="1">
        <label class="md-nav__title" for="__nav_6">
          <span class="md-nav__icon md-icon"></span>
          <span class="md-ellipsis">Kenning gallery of use cases</span>
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
  
    <li class="md-nav__item">
      <a href="anomaly-detection-automl.html" class="md-nav__link">
        <span class="md-ellipsis">Generating anomaly detection models for the MAX32690 Evaluation Kit with Auto<wbr>ML</span>
      </a>
    </li>
  

            
          
            
              
  
  
  
  
    <li class="md-nav__item">
      <a href="anomaly-detection-on-mcu.html" class="md-nav__link">
        <span class="md-ellipsis">Anomaly detection model training and deployment on the MAX32690 Evaluation Kit</span>
      </a>
    </li>
  

            
          
            
              
  
  
  
  
    <li class="md-nav__item">
      <a href="displaying-information-example.html" class="md-nav__link">
        <span class="md-ellipsis">Displaying information about available classes</span>
      </a>
    </li>
  

            
          
            
              
  
  
  
  
    <li class="md-nav__item">
      <a href="kenning-zephyr-runtime.html" class="md-nav__link">
        <span class="md-ellipsis">Evaluating models on hardware using Kenning Zephyr Runtime</span>
      </a>
    </li>
  

            
          
            
              
  
  
  
  
    <li class="md-nav__item">
      <a href="pipeline-manager-example.html" class="md-nav__link">
        <span class="md-ellipsis">Visualizing Kenning data flows with Pipeline Manager</span>
      </a>
    </li>
  

            
          
            
              
  
  
  
  
    <li class="md-nav__item">
      <a href="renode-integration-example.html" class="md-nav__link">
        <span class="md-ellipsis">Bare-<wbr>metal IREE runtime simulated using Renode</span>
      </a>
    </li>
  

            
          
            
              
  
  
  
  
    <li class="md-nav__item">
      <a href="structured-pruning-torch.html" class="md-nav__link">
        <span class="md-ellipsis">Structured pruning for Py<wbr>Torch models</span>
      </a>
    </li>
  

            
          
            
              
  
  
    
  
    
  
  
    <li class="md-nav__item md-nav__item--active">
      
      
      <input class="md-nav__toggle md-toggle" data-md-toggle="toc" type="checkbox" id="__toc">
      
        <label class="md-nav__link md-nav__link--active" for="__toc">
          <span class="md-ellipsis">Model quantization and compilation using TFLite and TVM</span>
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <a href="#" class="md-nav__link md-nav__link--active">
        <span class="md-ellipsis">Model quantization and compilation using TFLite and TVM</span>
      </a>
      
        

<nav class="md-nav md-nav--secondary">
  
  
    
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#prepare-environment" class="md-nav__link">
    <span class="md-ellipsis">Prepare environment</span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#train-the-model-optional" class="md-nav__link">
    <span class="md-ellipsis">Train the model <wbr>(optional)</span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#benchmarking-a-model-using-a-native-framework" class="md-nav__link">
    <span class="md-ellipsis">Benchmarking a model using a native framework</span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#optimizing-a-model-using-tensorflow-lite" class="md-nav__link">
    <span class="md-ellipsis">Optimizing a model using Tensor<wbr>Flow Lite</span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#quantizing-a-model-using-tensorflow-lite" class="md-nav__link">
    <span class="md-ellipsis">Quantizing a model using Tensor<wbr>Flow Lite</span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#speeding-up-inference-with-apache-tvm" class="md-nav__link">
    <span class="md-ellipsis">Speeding up inference with Apache TVM</span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#automated-model-comparison" class="md-nav__link">
    <span class="md-ellipsis">Automated model comparison</span>
  </a>
  
</li>
      
    </ul>
  
</nav>
      
    </li>
  

            
          
            
              
  
  
  
  
    <li class="md-nav__item">
      <a href="unstructured-pruning-tensorflow.html" class="md-nav__link">
        <span class="md-ellipsis">Unstructured Pruning of Tensor<wbr>Flow Models</span>
      </a>
    </li>
  

            
          
            
              
  
  
  
  
    <li class="md-nav__item">
      <a href="yolact-object-detection-comparison.html" class="md-nav__link">
        <span class="md-ellipsis">Optimizing and comparing instance segmentation models</span>
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  
  

    
      
      
      

  
  
  
  
    <li class="md-nav__item">
      <a href="../kenning-variables.html" class="md-nav__link">
        <span class="md-ellipsis">Kenning environment variables</span>
      </a>
    </li>
  

    
      
      
      

  
  
  
  
    <li class="md-nav__item">
      <a href="../kenning-measurements.html" class="md-nav__link">
        <span class="md-ellipsis">Kenning measurements</span>
      </a>
    </li>
  

    
      
      
      

  
  
  
  
    <li class="md-nav__item">
      <a href="../pipeline-optimizer.html" class="md-nav__link">
        <span class="md-ellipsis">Choosing optimal optimization pipeline</span>
      </a>
    </li>
  

    
      
      
      

  
  
  
  
    <li class="md-nav__item">
      <a href="../sample-report.html" class="md-nav__link">
        <span class="md-ellipsis">Sample autogenerated report</span>
      </a>
    </li>
  

    
      
      
      

  
  
  
  
    <li class="md-nav__item">
      <a href="../sample-automl-report.html" class="md-nav__link">
        <span class="md-ellipsis">Sample Auto<wbr>ML report</span>
      </a>
    </li>
  

    
      
      
      

  
  
  
  
    <li class="md-nav__item">
      <a href="../kenning-flow.html" class="md-nav__link">
        <span class="md-ellipsis">Creating applications with Kenning</span>
      </a>
    </li>
  

    
      
      
      

  
  
  
  
    <li class="md-nav__item">
      <a href="../kenning-development.html" class="md-nav__link">
        <span class="md-ellipsis">Developing Kenning blocks</span>
      </a>
    </li>
  

    
      
      
      

  
  
  
  
    <li class="md-nav__item">
      <a href="../kenning-resources.html" class="md-nav__link">
        <span class="md-ellipsis">Kenning resources</span>
      </a>
    </li>
  

    
      
      
      

  
  
  
  
    <li class="md-nav__item">
      <a href="../kenning-protocols.html" class="md-nav__link">
        <span class="md-ellipsis">Kenning protocols</span>
      </a>
    </li>
  

    
      
      
      

  
  
  
  
    <li class="md-nav__item">
      <a href="../kenning-platforms.html" class="md-nav__link">
        <span class="md-ellipsis">Kenning platforms</span>
      </a>
    </li>
  

    
      
      
      

  
  
  
  
    <li class="md-nav__item">
      <a href="../kenning-api.html" class="md-nav__link">
        <span class="md-ellipsis">Kenning API</span>
      </a>
    </li>
  

    
  </ul>
</nav>
                  </div>
                </div>
              </div>
            
            
          
          
            <div class="md-content" data-md-component="content">
              <article class="md-content__inner md-typeset" role="main">
                  


  <a href="https://github.com/antmicro/kenning/blob/main/docs/source/gallery/tflite-tvm.md" title="Edit this page" class="md-content__button md-icon">
    
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20.71 7.04c.39-.39.39-1.04 0-1.41l-2.34-2.34c-.37-.39-1.02-.39-1.41 0l-1.84 1.83 3.75 3.75M3 17.25V21h3.75L17.81 9.93l-3.75-3.75L3 17.25Z"/></svg>
  </a>

<h1 id="model-quantization-and-compilation-using-tflite-and-tvm">Model quantization and compilation using TFLite and TVM<a class="headerlink" href="#model-quantization-and-compilation-using-tflite-and-tvm" title="Link to this heading">¶</a></h1>
<p>Let’s consider a simple scenario where we want to optimize the inference time and memory usage of a classification model executed on a x86 CPU.</p>
<p>To do this, we are going to use the <a class="reference external" href="https://github.com/antmicro/kenning/blob/main/kenning/datasets/pet_dataset.py"><code class="docutils literal notranslate"><span class="pre">PetDataset</span></code></a> Dataset and the <a class="reference external" href="https://github.com/antmicro/kenning/blob/main/kenning/modelwrappers/classification/tensorflow_pet_dataset.py"><code class="docutils literal notranslate"><span class="pre">TensorFlowPetDatasetMobileNetV2</span></code></a> ModelWrapper.</p>
<h2 id="prepare-environment">Prepare environment<a class="headerlink" href="#prepare-environment" title="Link to this heading">¶</a></h2>
<p>First of all, we need to install all necessary dependencies:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><code>pip<span class="w"> </span>install<span class="w"> </span><span class="s2">&quot;kenning[tensorflow,tflite,tvm,reports] @ git+https://github.com/antmicro/kenning.git&quot;</span>
</code></pre></div>
</div>
<h2 id="train-the-model-optional">Train the model (optional)<a class="headerlink" href="#train-the-model-optional" title="Link to this heading">¶</a></h2>
<p>We will skip the training process, we will use <a class="reference external" href="https://dl.antmicro.com/kenning/models/classification/tensorflow_pet_dataset_mobilenetv2.h5">tensorflow_pet_dataset_mobilenetv2.h5</a>.
In Kenning, available models and resources can be downloaded using URIs with the <code class="docutils literal notranslate"><span class="pre">kenning://</span></code> scheme, in this case <code class="docutils literal notranslate"><span class="pre">kenning:///models/classification/tensorflow_pet_dataset_mobilenetv2.h5</span></code>.</p>
<p>The training of the above model can be performed using the following command:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><code>kenning<span class="w"> </span>train<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--modelwrapper-cls<span class="w"> </span>kenning.modelwrappers.classification.tensorflow_pet_dataset.TensorFlowPetDatasetMobileNetV2<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--dataset-cls<span class="w"> </span>kenning.datasets.pet_dataset.PetDataset<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--logdir<span class="w"> </span>build/logs<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--dataset-root<span class="w"> </span>build/pet-dataset<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--model-path<span class="w"> </span>build/trained-model.h5<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--batch-size<span class="w"> </span><span class="m">32</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--learning-rate<span class="w"> </span><span class="m">0</span>.0001<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--num-epochs<span class="w"> </span><span class="m">50</span>
</code></pre></div>
</div>
<h2 id="benchmarking-a-model-using-a-native-framework">Benchmarking a model using a native framework<a class="headerlink" href="#benchmarking-a-model-using-a-native-framework" title="Link to this heading">¶</a></h2>
<p>First, we want to check how the trained model performs using the native framework on CPU.</p>
<p>For this, we will use the <code class="docutils literal notranslate"><span class="pre">kenning</span> <span class="pre">test</span></code> tool.
The tool is configured with JSON files (scenarios).</p>
<p>In our case, the JSON file (named <code class="docutils literal notranslate"><span class="pre">native.json</span></code>) will look like this:</p>
<div class="literal-block-wrapper docutils container" id="id1">
<div class="code-block-caption highlight"><span class="filename"><span class="caption-number">Listing 8 </span><span class="caption-text">
</span>mobilenetv2-tensorflow-native.json<a class="headerlink" href="#id1" title="Permalink to this code">¶</a></span></div>
<div class="highlight-json notranslate"><div class="highlight"><pre><span></span><code><span class="p">{</span>
<span class="w">  </span><span class="nt">&quot;model_wrapper&quot;</span><span class="p">:</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="nt">&quot;type&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;kenning.modelwrappers.classification.tensorflow_pet_dataset.TensorFlowPetDatasetMobileNetV2&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="nt">&quot;parameters&quot;</span><span class="p">:</span><span class="w"> </span><span class="p">{</span>
<span class="w">      </span><span class="nt">&quot;model_name&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;native&quot;</span><span class="p">,</span>
<span class="w">      </span><span class="nt">&quot;model_path&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;kenning:///models/classification/tensorflow_pet_dataset_mobilenetv2.h5&quot;</span><span class="p">,</span>
<span class="w">      </span><span class="nt">&quot;batch_size&quot;</span><span class="p">:</span><span class="w"> </span><span class="mi">32</span><span class="p">,</span>
<span class="w">      </span><span class="nt">&quot;learning_rate&quot;</span><span class="p">:</span><span class="w"> </span><span class="mf">0.0001</span><span class="p">,</span>
<span class="w">      </span><span class="nt">&quot;num_epochs&quot;</span><span class="p">:</span><span class="w"> </span><span class="mi">50</span><span class="p">,</span>
<span class="w">      </span><span class="nt">&quot;logdir&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;build/logs&quot;</span>
<span class="w">    </span><span class="p">}</span>
<span class="w">  </span><span class="p">},</span>
<span class="w">  </span><span class="nt">&quot;dataset&quot;</span><span class="p">:</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="nt">&quot;type&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;kenning.datasets.pet_dataset.PetDataset&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="nt">&quot;parameters&quot;</span><span class="p">:</span><span class="w"> </span><span class="p">{</span>
<span class="w">      </span><span class="nt">&quot;dataset_root&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;./build/PetDataset&quot;</span>
<span class="w">    </span><span class="p">}</span>
<span class="w">  </span><span class="p">}</span>
<span class="p">}</span>
</code></pre></div>
</div>
</div>
<p>This JSON provides a configuration for running the model natively and evaluating it against a defined Dataset.</p>
<p>For every class in the JSON file above, two keys are required: <code class="docutils literal notranslate"><span class="pre">type</span></code> which is a module path of our class and <code class="docutils literal notranslate"><span class="pre">parameters</span></code> which is used to provide arguments used to create instances of our classes.</p>
<p>In <code class="docutils literal notranslate"><span class="pre">model_wrapper</span></code>, we specify the model used for evaluation - here it is MobileNetV2 trained on PetDataset.
The <code class="docutils literal notranslate"><span class="pre">model_path</span></code> is the path to the saved model.
The <code class="docutils literal notranslate"><span class="pre">TensorFlowPetDatasetMobileNetV2</span></code> model wrapper provides methods for loading the model, preprocessing the inputs, postprocessing the outputs and running inference using the native framework (TensorFlow in this case).</p>
<p>The dataset provided for evaluation is PetDataset - here we specify that we want to download the dataset to the <code class="docutils literal notranslate"><span class="pre">./build/pet-dataset</span></code> directory (<code class="docutils literal notranslate"><span class="pre">dataset_root</span></code>).
The <code class="docutils literal notranslate"><span class="pre">PetDataset</span></code> class can download the dataset (if necessary), load it, read the inputs and outputs from files, process them, and implement evaluation methods for the model.</p>
<p>With the config above saved in the <code class="docutils literal notranslate"><span class="pre">native.json</span></code> file, run the <code class="docutils literal notranslate"><span class="pre">kenning</span> <span class="pre">test</span></code> scenario:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><code>kenning<span class="w"> </span><span class="nb">test</span><span class="w"> </span><span class="se">\</span>
<span class="w">  </span>--json-cfg<span class="w"> </span>mobilenetv2-tensorflow-native.json<span class="w"> </span><span class="se">\</span>
<span class="w">  </span>--measurements<span class="w"> </span>build/native.json
</code></pre></div>
</div>
<p>This tool runs inference based on the given configuration, evaluates the model and stores quality and performance metrics in JSON format, saved to the <code class="docutils literal notranslate"><span class="pre">build/native.json</span></code> file.
All other JSONs in this example use case can be executed with this command.</p>
<p>To visualize the evaluation and benchmark results, run the <code class="docutils literal notranslate"><span class="pre">kenning</span> <span class="pre">report</span></code> tool:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><code>kenning<span class="w"> </span>report<span class="w"> </span><span class="se">\</span>
<span class="w">  </span>--report-path<span class="w"> </span>build/benchmarks/native.md<span class="w"> </span><span class="se">\</span>
<span class="w">  </span>--measurements<span class="w"> </span>build/native.json<span class="w"> </span><span class="se">\</span>
<span class="w">  </span>--root-dir<span class="w"> </span>build/benchmarks<span class="w"> </span><span class="se">\</span>
<span class="w">  </span>--img-dir<span class="w"> </span>build/benchmarks/img<span class="w"> </span><span class="se">\</span>
<span class="w">  </span>--report-types<span class="w"> </span>performance<span class="w"> </span>classification<span class="w"> </span><span class="se">\</span>
<span class="w">  </span>--report-name<span class="w"> </span><span class="s1">&#39;native&#39;</span>
</code></pre></div>
</div>
<p>The <code class="docutils literal notranslate"><span class="pre">kenning</span> <span class="pre">report</span></code> tool takes the output JSON file generated by the <code class="docutils literal notranslate"><span class="pre">kenning</span> <span class="pre">test</span></code> tool, and creates a report titled <code class="docutils literal notranslate"><span class="pre">native</span></code>, which is saved in the <code class="docutils literal notranslate"><span class="pre">build/benchmarks/native.md</span></code> directory.
As specified in the <code class="docutils literal notranslate"><span class="pre">--report-types</span></code> flag, we create <code class="docutils literal notranslate"><span class="pre">performance</span></code> and <code class="docutils literal notranslate"><span class="pre">classification</span></code> metrics sections in the report (for example, there is also a <code class="docutils literal notranslate"><span class="pre">detection</span></code> report type for object detection tasks).</p>
<p>The <code class="docutils literal notranslate"><span class="pre">build/benchmarks/img</span></code> directory contains images with the <code class="docutils literal notranslate"><span class="pre">native_*</span></code> prefix visualizing the confusion matrix, CPU and memory usage, as well as inference time.</p>
<p>The <code class="docutils literal notranslate"><span class="pre">build/benchmarks/native.md</span></code> file is a Markdown document containing a full report for the model.
The document links to the generated visualizations and provides aggregated information about CPU and memory usage, as well as classification quality metrics, such as accuracy, sensitivity, precision, or G-Mean.
Such a file can be included in a larger, Sphinx-based documentation, which allows easy, automated report generation, using e.g. CI, like in the case of the <a class="reference external" href="https://antmicro.github.io/kenning/sample-report.html">Kenning documentation</a>.</p>
<p>We can also use a simplified command:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><code>kenning<span class="w"> </span>report<span class="w"> </span><span class="se">\</span>
<span class="w">  </span>--report-path<span class="w"> </span>build/benchmarks/native.md<span class="w"> </span><span class="se">\</span>
<span class="w">  </span>--measurements<span class="w"> </span>build/native.json
</code></pre></div>
</div>
<p>The available <code class="docutils literal notranslate"><span class="pre">--report-types</span></code> will be derived from measurements in the <code class="docutils literal notranslate"><span class="pre">build/native.json</span></code> file.</p>
<p>Conveniently, <code class="docutils literal notranslate"><span class="pre">kenning</span> <span class="pre">test</span></code> and <code class="docutils literal notranslate"><span class="pre">kenning</span> <span class="pre">report</span></code> commands can be reduced to a single <code class="docutils literal notranslate"><span class="pre">kenning</span></code> run:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><code>kenning<span class="w"> </span><span class="nb">test</span><span class="w"> </span>report<span class="w"> </span><span class="se">\</span>
<span class="w">  </span>--json-cfg<span class="w"> </span>mobilenetv2-tensorflow-native.json<span class="w"> </span><span class="se">\</span>
<span class="w">  </span>--measurements<span class="w"> </span>build/native.json<span class="w"> </span><span class="se">\</span>
<span class="w">  </span>--report-path<span class="w"> </span>build/benchmarks/native.md
</code></pre></div>
</div>
<p>While native frameworks are great for training and inference, model design, training on GPUs and distributing training across many devices, e.g. in a cloud environment, there is a fairly large variety of inference-focused frameworks for production purposes that focus on getting the most out of hardware in order to get results as fast as possible.</p>
<h2 id="optimizing-a-model-using-tensorflow-lite">Optimizing a model using TensorFlow Lite<a class="headerlink" href="#optimizing-a-model-using-tensorflow-lite" title="Link to this heading">¶</a></h2>
<p>TensorFlow Lite is one of such frameworks.
It is a lightweight library for inferring networks on edge - it has a small binary size (which can be reduced further by disabling unused operators) and a highly optimized format of input models, called FlatBuffers.</p>
<p>Before the TensorFlow Lite Interpreter (runtime for the TensorFlow Lite library) can be used, the model first needs to be optimized and compiled to the <code class="docutils literal notranslate"><span class="pre">.tflite</span></code> format.</p>
<p>Let’s add a TensorFlow Lite Optimizer that will convert our MobileNetV2 model to a FlatBuffer format, as well as a TensorFlow Lite Runtime that will execute the model:</p>
<div class="literal-block-wrapper docutils container" id="id2">
<div class="code-block-caption highlight"><span class="filename"><span class="caption-number">Listing 9 </span><span class="caption-text">
</span>mobilenetv2-tensorflow-tflite-f32.json<a class="headerlink" href="#id2" title="Permalink to this code">¶</a></span></div>
<div class="highlight-json notranslate"><div class="highlight"><pre><span></span><code><span class="w">    </span><span class="nt">&quot;optimizers&quot;</span><span class="p">:</span><span class="w"> </span><span class="p">[</span>
<span class="w">        </span><span class="p">{</span>
<span class="w">            </span><span class="nt">&quot;type&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;kenning.optimizers.tflite.TFLiteCompiler&quot;</span><span class="p">,</span>
<span class="w">            </span><span class="nt">&quot;parameters&quot;</span><span class="p">:</span><span class="w"> </span><span class="p">{</span>
<span class="w">                </span><span class="nt">&quot;target&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;default&quot;</span><span class="p">,</span>
<span class="w">                </span><span class="nt">&quot;compiled_model_path&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;./build/fp32.tflite&quot;</span><span class="p">,</span>
<span class="w">                </span><span class="nt">&quot;inference_input_type&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;float32&quot;</span><span class="p">,</span>
<span class="w">                </span><span class="nt">&quot;inference_output_type&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;float32&quot;</span>
<span class="w">            </span><span class="p">}</span>
<span class="w">        </span><span class="p">}</span>
</code></pre></div>
</div>
</div>
<p>The configuration of the already existing blocks does not change, and the dataset will not be downloaded again since the files are already present.</p>
<p>The first new addition in comparison to the previous flow is the presence of the <code class="docutils literal notranslate"><span class="pre">optimizers</span></code> list, which allows us to add one or more objects inheriting from the <code class="docutils literal notranslate"><span class="pre">kenning.core.optimizer.Optimizer</span></code> class.
Optimizers read the model from the input file, apply various optimizations, and then save the optimized model to a new file.</p>
<p>In our current scenario, we will use the <code class="docutils literal notranslate"><span class="pre">TFLiteCompiler</span></code> class - it reads the model in a Keras-specific format, optimizes the model and saves it to the <code class="docutils literal notranslate"><span class="pre">./build/fp32.tflite</span></code> file.
The parameters of this particular Optimizer are worth noting here (each Optimizer usually has a different set of parameters):</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">target</span></code> - indicates what the desired target device (or model type) is, regular CPU is <code class="docutils literal notranslate"><span class="pre">default</span></code>.
Another example here could be <code class="docutils literal notranslate"><span class="pre">edgetpu</span></code>, which can compile models for the Google Coral platform.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">compiled_model_path</span></code> - indicates where the model should be saved.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">inference_input_type</span></code> and <code class="docutils literal notranslate"><span class="pre">inference_output_type</span></code> - indicate what the input and output type of the model should be.
Usually, all trained models use FP32 weights (32-bit floating point) and activations - using <code class="docutils literal notranslate"><span class="pre">float32</span></code> here keeps the weights unchanged.</p></li>
</ul>
<p>Another addition is the <code class="docutils literal notranslate"><span class="pre">runtime</span></code> block which provides a class inheriting from the <code class="docutils literal notranslate"><span class="pre">kenning.core.runtime.Runtime</span></code> class that is able to load the final model and run inference on target hardware.
Usually, each <code class="docutils literal notranslate"><span class="pre">Optimizer</span></code> has a corresponding <code class="docutils literal notranslate"><span class="pre">Runtime</span></code> capable of running its results.</p>
<p>To compile the scenario (called <code class="docutils literal notranslate"><span class="pre">tflite-fp32.json</span></code>), run:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><code>kenning<span class="w"> </span>optimize<span class="w"> </span><span class="nb">test</span><span class="w"> </span>report<span class="w"> </span><span class="se">\</span>
<span class="w">  </span>--json-cfg<span class="w"> </span>mobilenetv2-tensorflow-tflite-f32.json<span class="w"> </span><span class="se">\</span>
<span class="w">  </span>--measurements<span class="w"> </span>build/tflite-fp32.json<span class="w"> </span><span class="se">\</span>
<span class="w">  </span>--report-path<span class="w"> </span>build/benchmarks/tflite-fp32.md
</code></pre></div>
</div>
<p>While it depends on the platform used, you should be able to see a significant improvement in both inference time (model ca. 10-15x faster model compared to the native model) and memory usage (output model ca. 2x smaller).
What’s worth noting is that we get a significant improvement with no harm to the quality of the model - the outputs stay the same.</p>
<p><img alt="Confusion matrix" src="../_images/confusion-matrix.png" /></p>
<h2 id="quantizing-a-model-using-tensorflow-lite">Quantizing a model using TensorFlow Lite<a class="headerlink" href="#quantizing-a-model-using-tensorflow-lite" title="Link to this heading">¶</a></h2>
<p>To further reduce memory usage, we can quantize the model - it is a process where all weights and activations in a model are calibrated to work with low-precision floating-point or integer reporesentations (i.e. <code class="docutils literal notranslate"><span class="pre">INT8</span></code>), instead of the <code class="docutils literal notranslate"><span class="pre">FP32</span></code> precision.
While it may severely harm prediction quality, the quality reduction can be negligible with proper calibration.</p>
<p>The model can be quantized during the compilation process in TensorFlow Lite.
With Kenning, it can be achieved with the following simple additions:</p>
<div class="literal-block-wrapper docutils container" id="id3">
<div class="code-block-caption highlight"><span class="filename"><span class="caption-number">Listing 10 </span><span class="caption-text">
</span>mobilenetv2-tensorflow-tflite-int8.json<a class="headerlink" href="#id3" title="Permalink to this code">¶</a></span></div>
<div class="highlight-json notranslate"><div class="highlight"><pre><span></span><code><span class="w">  </span><span class="nt">&quot;optimizers&quot;</span><span class="p">:</span><span class="w"> </span><span class="p">[</span>
<span class="w">    </span><span class="p">{</span>
<span class="w">      </span><span class="nt">&quot;type&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;kenning.optimizers.tflite.TFLiteCompiler&quot;</span><span class="p">,</span>
<span class="w">      </span><span class="nt">&quot;parameters&quot;</span><span class="p">:</span><span class="w"> </span><span class="p">{</span>
<span class="w">        </span><span class="nt">&quot;target&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;int8&quot;</span><span class="p">,</span>
<span class="w">        </span><span class="nt">&quot;compiled_model_path&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;./build/int8.tflite&quot;</span><span class="p">,</span>
<span class="w">        </span><span class="nt">&quot;inference_input_type&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;int8&quot;</span><span class="p">,</span>
<span class="w">        </span><span class="nt">&quot;inference_output_type&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;int8&quot;</span>
<span class="w">      </span><span class="p">}</span>
<span class="w">    </span><span class="p">}</span>
</code></pre></div>
</div>
</div>
<p>The only changes here in comparison to the previous configuration appear in the <code class="docutils literal notranslate"><span class="pre">TFLiteCompiler</span></code> configuration. We change <code class="docutils literal notranslate"><span class="pre">target</span></code>, <code class="docutils literal notranslate"><span class="pre">inference_input_type</span></code> and <code class="docutils literal notranslate"><span class="pre">inference_output_type</span></code> to <code class="docutils literal notranslate"><span class="pre">int8</span></code>.
Then, in the background, <code class="docutils literal notranslate"><span class="pre">TFLiteCompiler</span></code> fetches a subset of images from the <code class="docutils literal notranslate"><span class="pre">PetDataset</span></code> object to calibrate the model and the entire model calibration process happens automatically.</p>
<p>Let’s run the scenario above (<code class="docutils literal notranslate"><span class="pre">tflite-int8.json</span></code>):</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><code>kenning<span class="w"> </span>optimize<span class="w"> </span><span class="nb">test</span><span class="w"> </span>report<span class="w"> </span><span class="se">\</span>
<span class="w">  </span>--json-cfg<span class="w"> </span>mobilenetv2-tensorflow-tflite-int8.json<span class="w"> </span><span class="se">\</span>
<span class="w">  </span>--measurements<span class="w"> </span>build/tflite-int8.json<span class="w"> </span><span class="se">\</span>
<span class="w">  </span>--report-path<span class="w"> </span>build/benchmarks/tflite-int8.md
</code></pre></div>
</div>
<p>This results in a model over 7 times smaller compared to the native model without significant loss of accuracy (but without speed improvement).</p>
<h2 id="speeding-up-inference-with-apache-tvm">Speeding up inference with Apache TVM<a class="headerlink" href="#speeding-up-inference-with-apache-tvm" title="Link to this heading">¶</a></h2>
<p>To speed up inference of a quantized model, we can utilize vector extensions in x86 CPUs, more specifically AVX2.
To do this, we can use the Apache TVM framework to compile efficient runtimes for various hardware platforms.
The scenario looks like this:</p>
<div class="literal-block-wrapper docutils container" id="id4">
<div class="code-block-caption highlight"><span class="filename"><span class="caption-number">Listing 11 </span><span class="caption-text">
</span>mobilenetv2-tensorflow-tvm-avx-int8.json<a class="headerlink" href="#id4" title="Permalink to this code">¶</a></span></div>
<div class="highlight-json notranslate"><div class="highlight"><pre><span></span><code><span class="w">  </span><span class="nt">&quot;optimizers&quot;</span><span class="p">:</span><span class="w"> </span><span class="p">[</span>
<span class="w">    </span><span class="p">{</span>
<span class="w">      </span><span class="nt">&quot;type&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;kenning.optimizers.tflite.TFLiteCompiler&quot;</span><span class="p">,</span>
<span class="w">      </span><span class="nt">&quot;parameters&quot;</span><span class="p">:</span><span class="w"> </span><span class="p">{</span>
<span class="w">        </span><span class="nt">&quot;target&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;int8&quot;</span><span class="p">,</span>
<span class="w">        </span><span class="nt">&quot;compiled_model_path&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;./build/int8.tflite&quot;</span><span class="p">,</span>
<span class="w">        </span><span class="nt">&quot;inference_input_type&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;int8&quot;</span><span class="p">,</span>
<span class="w">        </span><span class="nt">&quot;inference_output_type&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;int8&quot;</span>
<span class="w">      </span><span class="p">}</span>
<span class="w">    </span><span class="p">},</span>
<span class="w">    </span><span class="p">{</span>
<span class="w">      </span><span class="nt">&quot;type&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;kenning.optimizers.tvm.TVMCompiler&quot;</span><span class="p">,</span>
<span class="w">      </span><span class="nt">&quot;parameters&quot;</span><span class="p">:</span><span class="w"> </span><span class="p">{</span>
<span class="w">        </span><span class="nt">&quot;target&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;llvm -mcpu=core-avx2&quot;</span><span class="p">,</span>
<span class="w">        </span><span class="nt">&quot;opt_level&quot;</span><span class="p">:</span><span class="w"> </span><span class="mi">3</span><span class="p">,</span>
<span class="w">        </span><span class="nt">&quot;conv2d_data_layout&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;NCHW&quot;</span><span class="p">,</span>
<span class="w">        </span><span class="nt">&quot;compiled_model_path&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;./build/int8_tvm.tar&quot;</span>
<span class="w">      </span><span class="p">}</span>
<span class="w">    </span><span class="p">}</span>
</code></pre></div>
</div>
</div>
<p>As visible, adding a new framework is just a matter of simply adding and configuring another optimizer and using a  <code class="docutils literal notranslate"><span class="pre">Runtime</span></code> corresponding to the final <code class="docutils literal notranslate"><span class="pre">Optimizer</span></code>.</p>
<p>The <code class="docutils literal notranslate"><span class="pre">TVMCompiler</span></code>, with <code class="docutils literal notranslate"><span class="pre">llvm</span> <span class="pre">-mcpu=core-avx2</span></code> as the target, optimizes and compiles the model to use vector extensions.
The final result is a <code class="docutils literal notranslate"><span class="pre">.tar</span></code> file containing a shared library that implements the entire model.</p>
<p>Let’s compile the scenario (<code class="docutils literal notranslate"><span class="pre">tvm-avx2-int8.json</span></code>):</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><code>kenning<span class="w"> </span>optimize<span class="w"> </span><span class="nb">test</span><span class="w"> </span>report<span class="w"> </span><span class="se">\</span>
<span class="w">  </span>--json-cfg<span class="w"> </span>mobilenetv2-tensorflow-tvm-avx-int8.json<span class="w"> </span><span class="se">\</span>
<span class="w">  </span>--measurements<span class="w"> </span>build/tvm-avx2-int8.json<span class="w"> </span><span class="se">\</span>
<span class="w">  </span>--report-path<span class="w"> </span>build/benchmarks/tvm-avx2-int8.md
</code></pre></div>
</div>
<p>This results in a model over 40 times faster compared to the native implementation, with a 3x reduction in size.</p>
<p>This demonstrates how easily we can interconnect various frameworks and get the most out of hardware using Kenning, while performing just minor alterations to the configuration file.</p>
<p>The summary of passes looks as follows:</p>
<table class="docutils data align-default">
<thead>
<tr class="row-odd"><th class="head"><p></p></th>
<th class="head"><p>Speed boost</p></th>
<th class="head"><p>Accuracy</p></th>
<th class="head"><p>Size reduction</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>native</p></td>
<td><p>1</p></td>
<td><p>0.9572730984</p></td>
<td><p>1</p></td>
</tr>
<tr class="row-odd"><td><p>tflite-fp32</p></td>
<td><p>15.79405698</p></td>
<td><p>0.9572730984</p></td>
<td><p>1.965973551</p></td>
</tr>
<tr class="row-even"><td><p>tflite-int8</p></td>
<td><p>1.683232669</p></td>
<td><p>0.9519662539</p></td>
<td><p>7.02033412</p></td>
</tr>
<tr class="row-odd"><td><p>tvm-avx2-int8</p></td>
<td><p>41.61514549</p></td>
<td><p>0.9487005035</p></td>
<td><p>3.229375069</p></td>
</tr>
</tbody>
</table>
<h2 id="automated-model-comparison">Automated model comparison<a class="headerlink" href="#automated-model-comparison" title="Link to this heading">¶</a></h2>
<p>The <code class="docutils literal notranslate"><span class="pre">kenning</span> <span class="pre">report</span></code> tool also allows us to compare evaluation results for multiple models.
Apart from creating a model summary table, it also creates plots aggregating measurements collected during the evaluation process.</p>
<p>To create a comparison report for the above experiments, run:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><code>kenning<span class="w"> </span>report<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--report-path<span class="w"> </span>build/benchmarks/summary.md<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--measurements<span class="w"> </span><span class="se">\</span>
<span class="w">        </span>build/native.json<span class="w"> </span><span class="se">\</span>
<span class="w">        </span>build/tflite-fp32.json<span class="w"> </span><span class="se">\</span>
<span class="w">        </span>build/tflite-int8.json<span class="w"> </span><span class="se">\</span>
<span class="w">        </span>build/tvm-avx2-int8.json
</code></pre></div>
</div>
<p>Some examples of comparisons between various models rendered with the script:</p>
<ul>
<li><p>Accuracy, inference time and model size comparison:</p>
<p><img alt="" src="../_images/accuracy-inference-time-comparison.png" /></p>
</li>
<li><p>Resource utilization distribution:</p>
<p><img alt="" src="../_images/utilization-comparison.png" /></p>
</li>
<li><p>Comparison of classification metrics:</p>
<p><img alt="" src="../_images/classification-metrics-comparison.png" /></p>
</li>
<li><p>And more</p></li>
</ul>


  <hr>
<div class="md-source-file">
  <small>
    
      Last update:
      2025-09-18
    
  </small>
</div>





                
              </article>
            </div>
          
          
        </div>
        
      </main>
      
        <footer class="md-footer">
  
    
    <nav class="md-footer__inner md-grid" aria-label="Footer" >
      
        
        <a href="structured-pruning-torch.html" class="md-footer__link md-footer__link--prev" aria-label="Previous: Structured pruning for PyTorch models" rel="prev">
          <div class="md-footer__button md-icon">
            <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11h12Z"/></svg>
          </div>
          <div class="md-footer__title">
            <div class="md-ellipsis">
              <span class="md-footer__direction">
                Previous
              </span>
              Structured pruning for PyTorch models
            </div>
          </div>
        </a>
      
      
        
        <a href="unstructured-pruning-tensorflow.html" class="md-footer__link md-footer__link--next" aria-label="Next: Unstructured Pruning of TensorFlow Models" rel="next">
          <div class="md-footer__title">
            <div class="md-ellipsis">
              <span class="md-footer__direction">
                Next
              </span>
              Unstructured Pruning of TensorFlow Models
            </div>
          </div>
          <div class="md-footer__button md-icon">
            <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M4 11v2h12l-5.5 5.5 1.42 1.42L19.84 12l-7.92-7.92L10.5 5.5 16 11H4Z"/></svg>
          </div>
        </a>
      
    </nav>
  
  
  
  <div class="md-footer-meta md-typeset">
    
    <div class="md-footer-meta__inner md-grid">
      <div class="md-copyright">
  
    <div class="md-footer-copyright__highlight">
        Copyright &#169; 2020-2025, Antmicro.
        
    </div>
  
      <a href="https://github.com/antmicro/kenning/tree/e1e0e791de694af6c5b6c9d532b0700a742af703">e1e0e791</a>
        @ <a href="https://github.com/antmicro/kenning/tree/main">main</a>
  
</div>
      
        <div class="md-social">
  
    
    
      
      
    
    <a href="https://github.com/antmicro" target="_blank" rel="noopener" title="github.com" class="md-social__link">
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 496 512"><!--! Font Awesome Free 6.2.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2022 Fonticons, Inc.--><path d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6zm-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3zm44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9zM244.8 8C106.1 8 0 113.3 0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8zM97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1zm-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7zm32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1zm-11.4-14.7c-1.6 1-1.6 3.6 0 5.9 1.6 2.3 4.3 3.3 5.6 2.3 1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2z"/></svg>
    </a>
  
    
    
      
      
    
    <a href="https://twitter.com/antmicro" target="_blank" rel="noopener" title="twitter.com" class="md-social__link">
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><!--! Font Awesome Free 6.2.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2022 Fonticons, Inc.--><path d="M459.37 151.716c.325 4.548.325 9.097.325 13.645 0 138.72-105.583 298.558-298.558 298.558-59.452 0-114.68-17.219-161.137-47.106 8.447.974 16.568 1.299 25.34 1.299 49.055 0 94.213-16.568 130.274-44.832-46.132-.975-84.792-31.188-98.112-72.772 6.498.974 12.995 1.624 19.818 1.624 9.421 0 18.843-1.3 27.614-3.573-48.081-9.747-84.143-51.98-84.143-102.985v-1.299c13.969 7.797 30.214 12.67 47.431 13.319-28.264-18.843-46.781-51.005-46.781-87.391 0-19.492 5.197-37.36 14.294-52.954 51.655 63.675 129.3 105.258 216.365 109.807-1.624-7.797-2.599-15.918-2.599-24.04 0-57.828 46.782-104.934 104.934-104.934 30.213 0 57.502 12.67 76.67 33.137 23.715-4.548 46.456-13.32 66.599-25.34-7.798 24.366-24.366 44.833-46.132 57.827 21.117-2.273 41.584-8.122 60.426-16.243-14.292 20.791-32.161 39.308-52.628 54.253z"/></svg>
    </a>
  
</div>
      
    </div>
    
  </div>
</footer>
      
    </div>
    <div class="md-dialog" data-md-component="dialog">
      <div class="md-dialog__inner md-typeset"></div>
    </div>
    
    <script id="__config" type="application/json">{"base": "..", "features": ["toc.integrate"], "translations": {"clipboard.copied": "Copied to clipboard", "clipboard.copy": "Copy to clipboard", "search.config.lang": "en", "search.config.pipeline": "trimmer, stopWordFilter", "search.config.separator": "[\\s\\-]+", "search.placeholder": "Search", "search.result.more.one": "1 more on this page", "search.result.more.other": "# more on this page", "search.result.none": "No matching documents", "search.result.one": "1 matching document", "search.result.other": "# matching documents", "search.result.placeholder": "Type to start searching", "search.result.term.missing": "Missing", "select.version.title": "Select version"}}</script>
    
      
        <script src="../_static/sphinx_immaterial_theme.f9d9eeeb247ace16c.min.js?v=8ec58cb5"></script>
    
  </body>
</html>